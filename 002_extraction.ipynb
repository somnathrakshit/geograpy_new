{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geograpy3.extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import nltk\n",
    "import re\n",
    "from newspaper import Article\n",
    "from geograpy_new.labels import Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Extractor(object):\n",
    "    '''\n",
    "    Extract geo context for text or from url\n",
    "    '''\n",
    "    def __init__(self, text=None, url=None, debug=False):\n",
    "        '''\n",
    "        Constructor\n",
    "        Args:\n",
    "            text(string): the text to analyze\n",
    "            url(string): the url to read the text to analyze from\n",
    "            debug(boolean): if True show debug information\n",
    "        '''\n",
    "        if not text and not url:\n",
    "            raise Exception('text or url is required')\n",
    "        self.debug=debug\n",
    "        self.text = text\n",
    "        self.url = url\n",
    "        self.places = []\n",
    "        nltk_packages = ['maxent_ne_chunker',\n",
    "                        'words',\n",
    "                        'treebank',\n",
    "                        'maxent_treebank_pos_tagger',\n",
    "                        'punkt',\n",
    "                        'averaged_perceptron_tagger'\n",
    "                        ]\n",
    "        for nltk_package in nltk_packages:\n",
    "            try:\n",
    "                import nltk\n",
    "                nltk.data.find(nltk_package)\n",
    "            except LookupError:\n",
    "                nltk.downloader.download(nltk_package, quiet=True)\n",
    "        import nltk  \n",
    "\n",
    "    def set_text(self):\n",
    "        '''\n",
    "        Setter for text\n",
    "        '''\n",
    "        if not self.text and self.url:\n",
    "            a = Article(self.url)\n",
    "            a.download()\n",
    "            a.parse()\n",
    "            self.text = a.text\n",
    "            \n",
    "    def split(self,delimiter=r\",\"):\n",
    "        '''\n",
    "        simpler regular expression splitter with not entity check\n",
    "        \n",
    "        hat tip: https://stackoverflow.com/a/1059601/1497139\n",
    "        '''\n",
    "        self.set_text()\n",
    "        self.places=re.split(delimiter,self.text)\n",
    "            \n",
    "    def find_geoEntities(self):\n",
    "        '''\n",
    "        Find geographic entities\n",
    "        \n",
    "        Returns:\n",
    "            list: \n",
    "                List of places\n",
    "        '''\n",
    "        self.find_entities(Labels.geo)\n",
    "        return self.places\n",
    "        \n",
    "    def find_entities(self,labels=Labels.default):\n",
    "        '''\n",
    "        Find entities with the given labels set self.places and returns it\n",
    "        \n",
    "        Args:\n",
    "            labels: \n",
    "                Labels: The labels to filter\n",
    "                \n",
    "        Returns:\n",
    "            list: \n",
    "                List of places\n",
    "        '''\n",
    "        self.set_text()\n",
    "\n",
    "        text = nltk.word_tokenize(self.text)\n",
    "        nes = nltk.ne_chunk(nltk.pos_tag(text))\n",
    "\n",
    "        for ne in nes:\n",
    "            if type(ne) is nltk.tree.Tree:\n",
    "                nelabel=ne.label()\n",
    "                if (nelabel in labels):\n",
    "                    leaves=ne.leaves()\n",
    "                    if self.debug:\n",
    "                        print(leaves)\n",
    "                    self.places.append(u' '.join([i[0] for i in leaves]))\n",
    "        return self.places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
